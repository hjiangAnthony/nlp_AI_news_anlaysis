{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Pandarallel will run on 8 workers.\n",
      "INFO: Pandarallel will use Memory file system to transfer data between the main process and workers.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from pprint import pprint\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "import numpy as np\n",
    "\n",
    "from pandarallel import pandarallel\n",
    "pandarallel.initialize(progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/anthony/projects/nlp_runtime\n"
     ]
    }
   ],
   "source": [
    "#!pwd\n",
    "import os\n",
    "\n",
    "# Google Bucket\n",
    "# file name checkpoint_0512_sent_split.parquet\n",
    "path_bucket = 'gs://msca-sp23-bucket/nlp_data'\n",
    "path_bucket_df = path_bucket + '/' + 'checkpoint_0512_sent_split.parquet'\n",
    "runtime_path = '/home/anthony/projects/nlp_runtime'\n",
    "\n",
    "os.chdir(runtime_path)\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', 500)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>date</th>\n",
       "      <th>language</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>text_split_filtered</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>http://en.people.cn/n3/2021/0318/c90000-9830122.html</td>\n",
       "      <td>2021-03-18</td>\n",
       "      <td>en</td>\n",
       "      <td>Artificial intelligence improves parking efficiency in Chinese cities - People's Daily Online</td>\n",
       "      <td>\\n\\nArtificial intelligence improves parking efficiency in Chinese cities - People's Daily Online\\n\\nHome\\nChina Politics\\nForeign Affairs\\nOpinions\\nVideo: We Are China\\nBusiness\\nMilitary\\nWorld\\nSociety\\nCulture\\nTravel\\nScience\\nSports\\nPhoto\\n\\nLanguages\\n\\nChinese\\nJapanese\\nFrench\\nSpanish\\nRussian\\nArabic\\nKorean\\nGerman\\nPortuguese\\nThursday, March 18, 2021\\nHome&gt;&gt;\\n\\t\\t\\nArtificial intelligence improves parking efficiency in Chinese cities\\nBy Liu Shiyao (People's Daily) 09:16, Mar...</td>\n",
       "      <td>[Chinese Japanese French Spanish Russian Arabic Korean German Portuguese Thursday, March 18, 2021 Home     Artificial intelligence improves parking efficiency in Chinese cities By Liu Shiyao  People's Daily  09:16, March 18, 2021 Photo taken on July 1, 2019, shows a sign for electronic toll collection  ETC  newly set up at a roadside parking space on Yangzhuang road, Shijingshan district, Beijing. Some urban areas of the city started to use ETC system for roadside parking spaces since July 1...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    url        date language   \n",
       "0  http://en.people.cn/n3/2021/0318/c90000-9830122.html  2021-03-18       en  \\\n",
       "\n",
       "                                                                                           title   \n",
       "0  Artificial intelligence improves parking efficiency in Chinese cities - People's Daily Online  \\\n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  text   \n",
       "0  \\n\\nArtificial intelligence improves parking efficiency in Chinese cities - People's Daily Online\\n\\nHome\\nChina Politics\\nForeign Affairs\\nOpinions\\nVideo: We Are China\\nBusiness\\nMilitary\\nWorld\\nSociety\\nCulture\\nTravel\\nScience\\nSports\\nPhoto\\n\\nLanguages\\n\\nChinese\\nJapanese\\nFrench\\nSpanish\\nRussian\\nArabic\\nKorean\\nGerman\\nPortuguese\\nThursday, March 18, 2021\\nHome>>\\n\\t\\t\\nArtificial intelligence improves parking efficiency in Chinese cities\\nBy Liu Shiyao (People's Daily) 09:16, Mar...  \\\n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   text_split_filtered  \n",
       "0  [Chinese Japanese French Spanish Russian Arabic Korean German Portuguese Thursday, March 18, 2021 Home     Artificial intelligence improves parking efficiency in Chinese cities By Liu Shiyao  People's Daily  09:16, March 18, 2021 Photo taken on July 1, 2019, shows a sign for electronic toll collection  ETC  newly set up at a roadside parking space on Yangzhuang road, Shijingshan district, Beijing. Some urban areas of the city started to use ETC system for roadside parking spaces since July 1...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df.to_parquet('checkpoint_0512_sent_split.parquet', index=False)\n",
    "df = pd.read_parquet(path_bucket, engine='pyarrow')\n",
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(385796, 6)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Primary Filtering: Select Relevant Titles & Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use all titles to make a word cloud\n",
    "df_title = df[['title']]\n",
    "df_title.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WordCloud on topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "#% matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove punctuations in title\n",
    "df_title['title'] = df_title['title'].str.replace('[^\\w\\s]','')\n",
    "\n",
    "# only keep alphabets\n",
    "df_title['title'] = df_title['title'].str.replace('[^a-zA-Z]',' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \" \".join(title for title in df.title)\n",
    "print (\"There are {} words in the combination of all review.\".format(len(text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ref: https://www.datacamp.com/tutorial/wordcloud-python\n",
    "\n",
    "# Create stopword list:\n",
    "stopwords = set(STOPWORDS)\n",
    "stopwords.update([\"drink\", \"now\", \"wine\", \"flavor\", \"flavors\"])\n",
    "\n",
    "# Generate a word cloud image\n",
    "wordcloud = WordCloud(stopwords=stopwords, background_color=\"white\", width=800, height=400).generate(text)\n",
    "\n",
    "# Display the generated image:\n",
    "# the matplotlib way (set dpi=100)\n",
    "plt.figure(dpi=100)\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a WordCloud object\n",
    "wc = WordCloud(stopwords=stopwords, background_color=\"white\")\n",
    "\n",
    "# Generate the word frequencies from the text\n",
    "word_frequencies = wc.process_text(text)\n",
    "\n",
    "# Convert the word frequencies to a list of tuples\n",
    "word_list = list(word_frequencies.items())\n",
    "\n",
    "# Sort the word list by frequency\n",
    "sorted_word_list = sorted(word_list, key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check\n",
    "print(len(sorted_word_list))\n",
    "sorted_word_list[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualze the top 100 words and frequencies using a horizontal barplot\n",
    "import seaborn as sns\n",
    "\n",
    "# create a dataframe\n",
    "df_word = pd.DataFrame(sorted_word_list[:100], columns=['word', 'freq'])\n",
    "df_word.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot\n",
    "plt.figure(figsize=(10, 20))\n",
    "sns.barplot(x='freq', y='word', data=df_word)\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_word[df_word['word'] == 'Conversational AI']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering articles using title and text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect the top 22 words in the df_word; in lower cases\n",
    "wc_keywords = df_word['word'].str.lower().tolist()[:15]\n",
    "user_keywords = ['Big Data', 'Data Mining', 'Data Analytics', 'Data Visualization', 'Data Cleaning', 'Data Wrangling', 'Data Science', \n",
    "                 'Data Engineering', 'Data Governance', 'Data Security', 'Data Privacy', 'Data Ethics', 'Data Strategy', 'Data Operations', \n",
    "                 'Data Warehousing', 'Business Intelligence', 'Business Analytics', 'Predictive Analytics', 'Prescriptive Analytics', \n",
    "                 'Descriptive Analytics', 'Statistical Modeling', 'Machine Learning', 'Deep Learning', 'Neural Networks', 'Convolutional Neural Networks', \n",
    "                 'Recurrent Neural Networks', 'Generative Adversarial Networks', 'Natural Language Processing', 'Computer Vision', 'Image Processing', \n",
    "                 'Speech Recognition', 'Chatbots', 'Conversational AI', 'Autonomous Driving', 'Autonomous Car', 'Robotics', 'Reinforcement Learning', 'Transfer Learning', 'Model Deployment', \n",
    "                 'Model Monitoring', 'Model Interpretability', 'A/B Testing', 'Experimentation', 'Bias and Fairness in AI', 'Explainable AI', 'Human-in-the-Loop AI', \n",
    "                 'MLOps', 'CI/CD', 'Cloud Computing', 'Edge Computing', 'IoT', 'Blockchain', 'Privacy-Preserving Machine Learning', 'Federated Learning', 'Differential Privacy',\n",
    "                 'Secure Multi-Party Computation', 'Homomorphic Encryption', 'Data Governance Framework', 'Data Cataloging', 'Data Lineage', 'Data Profiling', 'Data Virtualization', \n",
    "                 'Data Integration', 'Data Federation', 'Data Architecture', 'Data Modeling', 'Data Encryption', 'Data Masking', 'Data Compliance', 'Data Stewardship', 'Data Science Workflow', \n",
    "                 'Data Pipelines', 'Text Analytics', 'Text Mining', 'Sentiment Analysis', 'Speech-to-Text', 'Text-to-Speech', 'OCR', 'Object Detection', 'Semantic Segmentation', 'Predictive Maintenance', 'Recommendation Systems']\n",
    "\n",
    "# convert to lower cases\n",
    "user_keywords = [x.lower() for x in user_keywords]\n",
    "\n",
    "# combine the two lists and convert to lower cases\n",
    "filter_keywords = wc_keywords + user_keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save filter keywords\n",
    "with open('word_list.txt', 'w') as f:\n",
    "    for word in filter_keywords:\n",
    "        f.write(word + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join the text_split_filtered back to a string\n",
    "df['text_rejoined'] = df['text_split_filtered'].str.join(' ')\n",
    "\n",
    "# change 'text_rejoined' and 'title' to str\n",
    "df['text_rejoined'] = df['text_rejoined'].astype(str)\n",
    "df['title'] = df['title'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save a checkpoint\n",
    "#df.to_parquet('checkpoint_0514_sent_split.parquet', index=False)\n",
    "#df = pd.read_parquet('checkpoint_0514_sent_split.parquet', engine='pyarrow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with title and text_rejoined, using parallel processing and ignore case; only keep those rows that contain at least one keyword\n",
    "'''#df_test = df.sample(1000)\n",
    "df_selected = df[df['title'].str.contains('|'.join(filter_keywords), case=False) |\n",
    "                      df['text_rejoined'].str.contains('|'.join(filter_keywords), case=False)]'''\n",
    "\n",
    "# Using pandaraellel for faster processing\n",
    "# Define the filtering function\n",
    "def filter_rows(row, filter_keywords):\n",
    "    for keyword in filter_keywords:\n",
    "        if keyword.lower() in row['title'].lower() or keyword.lower() in row['text_rejoined'].lower():\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "# Apply the function in parallel\n",
    "#df_test = df.sample(30)\n",
    "df_selected = df[df.parallel_apply(filter_rows, args=(filter_keywords,), axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the shape\n",
    "print('initial no.of articles', len(df))\n",
    "print('filtered no.of articles', len(df_selected))\n",
    "\n",
    "# show top 5 rows\n",
    "df_selected.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check before drop\n",
    "#df_selected.language.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save a copy of the original length of text so that we can compare later\n",
    "raw_text_length = df_selected['text'].apply(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop unwanted columns\n",
    "df_selected = df_selected.drop(columns=['language', 'text'])\n",
    "# rename columns\n",
    "df_selected = df_selected.rename(columns={'text_rejoined': 'text', 'text_split_filtered': 'text_split'})\n",
    "\n",
    "print(df_selected.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_selected.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save a cleaned copy to bucket\n",
    "#path_bucket_save = path_bucket + '/' + 'df_cleaned_0514.parquet'\n",
    "#df_selected.to_parquet(path_bucket_save, index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google Bucket\n",
    "# file name checkpoint_0512_sent_split.parquet\n",
    "\n",
    "path_bucket_df_cleaned = path_bucket + '/' + 'df_cleaned_0514.parquet'\n",
    "runtime_path = '/home/anthony/projects/nlp_runtime'\n",
    "\n",
    "os.chdir(runtime_path)\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.to_parquet('checkpoint_0512_sent_split.parquet', index=False)\n",
    "df = pd.read_parquet(path_bucket_df_cleaned, engine='pyarrow')\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sample(5)[['url', 'title', 'text']]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sample shows that we need more text cleaning:\n",
    "- Long, meaningless word: `ABuybacksLegalInterviewsManagementOfferingsIPOsInsider TradesBiotech/FDAFreightPoliticsGovernmentHealthcareMarkets`\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Define the filtering function\n",
    "def long_word_cleaner(text):\n",
    "    pattern = r'\\b\\w{13,}\\b|\\b\\w*[A-Z]{2,}\\w*\\b'\n",
    "    filtered_text = re.sub(pattern, '', text)\n",
    "    return filtered_text\n",
    "\n",
    "# Apply the function in parallel\n",
    "df['text_cleaned'] = df['text'].parallel_apply(lambda x: long_word_cleaner(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load previous text length\n",
    "df['raw_text_length'] = raw_text_length\n",
    "\n",
    "# new text length\n",
    "df['text_length'] = df['text_cleaned'].apply(lambda x: len(x))\n",
    "\n",
    "# reduce ratio\n",
    "df['reduce_ratio'] = (df['text_length'] / df['raw_text_length']) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['raw_text_length', 'text_length', 'reduce_ratio']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
