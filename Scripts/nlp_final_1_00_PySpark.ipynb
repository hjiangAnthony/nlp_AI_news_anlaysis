{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "90ef6902-60c9-460d-a1f4-18a503a404f1",
   "metadata": {},
   "source": [
    "Ref:\n",
    "\n",
    "1. https://stackoverflow.com/questions/55240940/error-while-installing-spark-on-google-colab\n",
    "2. https://github.com/WhitewaterFoundry/Pengwin/issues/448\n",
    "3. https://stackoverflow.com/questions/55595263/how-to-fix-no-filesystem-for-scheme-gs-in-pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a478d0d-b621-479f-9bcd-92936d9cb300",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a921c2a8-eed3-4a13-aecd-771e1f5dac86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "openjdk version \"11.0.18\" 2023-01-17\n",
      "OpenJDK Runtime Environment (build 11.0.18+10-post-Debian-1deb10u1)\n",
      "OpenJDK 64-Bit Server VM (build 11.0.18+10-post-Debian-1deb10u1, mixed mode, sharing)\n"
     ]
    }
   ],
   "source": [
    "#!sudo apt-get install default-jre default-jdk\n",
    "\n",
    "# set your spark folder to your system path environment. \n",
    "import os\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
    "#os.environ[\"PATH\"] = os.environ[\"JAVA_HOME\"] + \"/bin:\" + os.environ[\"PATH\"]\n",
    "! java -version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c4d79de-390f-4619-9e4a-82df39ebda23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae09e331-0364-4215-b479-ef7a66d9b848",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# start spark session\n",
    "#import sparknlp\n",
    "#import findspark\n",
    "#findspark.init()\n",
    "#spark = sparknlp.start()\n",
    "\n",
    "print(\"Spark NLP version\", sparknlp.version())\n",
    "print(\"Apache Spark version:\", spark.version)\n",
    "! cd ~/.ivy2/cache/com.johnsnowlabs.nlp/spark-nlp_2.12/jars && ls -lt'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8524874d-174d-467f-a3ec-d3cdad390f04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/05/09 06:25:53 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "23/05/09 06:25:53 WARN DependencyUtils: Local jar /path/to/gcs-connector-hadoop2-latest.jar does not exist, skipping.\n",
      "23/05/09 06:25:53 INFO SparkContext: Running Spark version 3.3.0\n",
      "23/05/09 06:25:53 INFO ResourceUtils: ==============================================================\n",
      "23/05/09 06:25:53 INFO ResourceUtils: No custom resources configured for spark.driver.\n",
      "23/05/09 06:25:53 INFO ResourceUtils: ==============================================================\n",
      "23/05/09 06:25:53 INFO SparkContext: Submitted application: pyspark-shell\n",
      "23/05/09 06:25:53 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
      "23/05/09 06:25:53 INFO ResourceProfile: Limiting resource is cpu\n",
      "23/05/09 06:25:53 INFO ResourceProfileManager: Added ResourceProfile id: 0\n",
      "23/05/09 06:25:53 INFO SecurityManager: Changing view acls to: jupyter\n",
      "23/05/09 06:25:53 INFO SecurityManager: Changing modify acls to: jupyter\n",
      "23/05/09 06:25:53 INFO SecurityManager: Changing view acls groups to: \n",
      "23/05/09 06:25:53 INFO SecurityManager: Changing modify acls groups to: \n",
      "23/05/09 06:25:53 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(jupyter); groups with view permissions: Set(); users  with modify permissions: Set(jupyter); groups with modify permissions: Set()\n",
      "23/05/09 06:25:54 INFO Utils: Successfully started service 'sparkDriver' on port 42775.\n",
      "23/05/09 06:25:54 INFO SparkEnv: Registering MapOutputTracker\n",
      "23/05/09 06:25:54 INFO SparkEnv: Registering BlockManagerMaster\n",
      "23/05/09 06:25:54 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
      "23/05/09 06:25:54 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
      "23/05/09 06:25:54 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
      "23/05/09 06:25:54 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-d3d5a626-de8b-4697-820a-14b8cfb4a912\n",
      "23/05/09 06:25:54 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB\n",
      "23/05/09 06:25:54 INFO SparkEnv: Registering OutputCommitCoordinator\n",
      "23/05/09 06:25:54 INFO Utils: Successfully started service 'SparkUI' on port 4040.\n",
      "23/05/09 06:25:54 ERROR SparkContext: Failed to add /path/to/gcs-connector-hadoop2-latest.jar to Spark environment\n",
      "java.io.FileNotFoundException: Jar /path/to/gcs-connector-hadoop2-latest.jar not found\n",
      "\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:1949)\n",
      "\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2004)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$new$12(SparkContext.scala:507)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$new$12$adapted(SparkContext.scala:507)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:507)\n",
      "\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\n",
      "\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n",
      "\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:238)\n",
      "\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n",
      "\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/05/09 06:25:54 INFO Executor: Starting executor ID driver on host umn-1683157271.us-central1-a.c.nlpfinal-385623.internal\n",
      "23/05/09 06:25:54 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''\n",
      "23/05/09 06:25:54 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 41799.\n",
      "23/05/09 06:25:54 INFO NettyBlockTransferService: Server created on umn-1683157271.us-central1-a.c.nlpfinal-385623.internal:41799\n",
      "23/05/09 06:25:54 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
      "23/05/09 06:25:54 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, umn-1683157271.us-central1-a.c.nlpfinal-385623.internal, 41799, None)\n",
      "23/05/09 06:25:54 INFO BlockManagerMasterEndpoint: Registering block manager umn-1683157271.us-central1-a.c.nlpfinal-385623.internal:41799 with 434.4 MiB RAM, BlockManagerId(driver, umn-1683157271.us-central1-a.c.nlpfinal-385623.internal, 41799, None)\n",
      "23/05/09 06:25:54 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, umn-1683157271.us-central1-a.c.nlpfinal-385623.internal, 41799, None)\n",
      "23/05/09 06:25:54 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, umn-1683157271.us-central1-a.c.nlpfinal-385623.internal, 41799, None)\n",
      "23/05/09 06:25:54 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.\n",
      "23/05/09 06:25:54 INFO SharedState: Warehouse path is 'file:/home/jupyter/nlp_final_scripts/spark-warehouse'.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# Create spark_session\n",
    "#spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .config(\"spark.jars\", \"/path/to/gcs-connector-hadoop2-latest.jar\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext\n",
    "spark.version\n",
    "\n",
    "spark.conf.set(\"spark.sql.repl.eagerEval.enabled\",True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eabb58bd-8973-4f7a-8fff-f6cd0b1329b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import storage\n",
    "\n",
    "# Reading data from open bucket\n",
    "#dataPath = 'gs://nlp_final_spark/check_points/checkpoint.parquet'\n",
    "dataPath = './data/checkpoint.parquet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c6c1a329-25b9-4ebe-a94e-7855de7ddbaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data\t\t\t\t       nlp_final_1_02_PySpark.ipynb\n",
      "nlp_final_1_01_data_preparation.ipynb  spark-3.3.2-bin-hadoop3\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1286bb4c-b043-413a-9dc5-5238b1def480",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data\n",
    "news_df = spark.read.parquet(dataPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb4e708b-f953-4e68-89c5-0224c509d0ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all files in given COS directory\n",
    "def list_blobs(bucket_name, folder_name):\n",
    "    gcs_client = storage.Client()\n",
    "    bucket = gcs_client.bucket(bucket_name)\n",
    "    blobs = list(bucket.list_blobs(prefix=folder_name))\n",
    "\n",
    "    for blob in blobs:\n",
    "        print(blob.name + '\\t' + str(blob.size))\n",
    "\n",
    "# List all files in given COS directory\n",
    "def list_blobs_pd(bucket_name, folder_name):\n",
    "    gcs_client = storage.Client()\n",
    "    bucket = gcs_client.bucket(bucket_name)\n",
    "    blobs = list(bucket.list_blobs(prefix=folder_name))\n",
    "\n",
    "    blob_name = []\n",
    "    blob_size = []\n",
    "    \n",
    "    for blob in blobs:\n",
    "        blob_name.append(blob.name)\n",
    "        blob_size.append(blob.size)\n",
    "\n",
    "    blobs_df = pd.DataFrame(list(zip(blob_name, blob_size)), columns=['Name','Size'])\n",
    "\n",
    "    blobs_df = blobs_df.style.format({\"Size\": \"{:,.0f}\"}) \n",
    "    \n",
    "    return blobs_df        \n",
    "\n",
    "# Delete folder from COS bucket\n",
    "def delete_folder(bucket_name, folder_name):\n",
    "    gcs_client = storage.Client()\n",
    "    bucket = gcs_client.bucket(bucket_name)\n",
    "    blobs = list(bucket.list_blobs(prefix=folder_name))\n",
    "\n",
    "    for blob in blobs:\n",
    "        blob.delete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f971c530-e094-4c39-9f34-7cce8aec3652",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving results into individual bucket, students must update to their own bucket `msca-bdp-students-bucket` and use `CNET ID` as a folder prefix\n",
    "bucket_write = 'nlp_final_spark'\n",
    "folder_write = 'check_points'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d65655-3789-45a9-bc2b-2084b6608548",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''#%%time\n",
    "data_joined.write.format('parquet').\\\n",
    "mode('overwrite').\\\n",
    "save('gs://' + bucket_write + '/' + folder_write)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "549d2c56-c62c-4292-a14f-c39352eb2c56",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "pytorch-gpu.1-13.m108",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-13:m108"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
