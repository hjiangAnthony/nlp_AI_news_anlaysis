{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Pandarallel will run on 8 workers.\n",
      "INFO: Pandarallel will use Memory file system to transfer data between the main process and workers.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from pprint import pprint\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "import numpy as np\n",
    "\n",
    "from pandarallel import pandarallel\n",
    "pandarallel.initialize(progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/anthony/projects/nlp_runtime\n"
     ]
    }
   ],
   "source": [
    "#!pwd\n",
    "import os\n",
    "\n",
    "# Google Bucket\n",
    "# file name checkpoint_0512_sent_split.parquet\n",
    "path_bucket = 'gs://msca-sp23-bucket/nlp_data'\n",
    "path_bucket_df_cleaned = path_bucket + '/' + 'df_cleaned_0514.parquet'\n",
    "runtime_path = '/home/anthony/projects/nlp_runtime'\n",
    "\n",
    "os.chdir(runtime_path)\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', 500)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim import corpora, models\n",
    "from gensim.models.ldamulticore import LdaMulticore\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models as gensimvis\n",
    "pyLDAvis.enable_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk as nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import spacy.cli\n",
    "spacy.cli.download(\"en_core_web_md\")\n",
    "nlp = spacy.load(\"en_core_web_md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data\n",
    "df_raw = pd.read_parquet(path_bucket_df_cleaned, engine='pyarrow')\n",
    "\n",
    "# take a sample\n",
    "df = df_raw.sample(150000, random_state=42)\n",
    "print(df.shape)\n",
    "df.head(1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA, on overall topics"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Prep\n",
    "\n",
    "I want to utilize parallelization as much as possile to save time"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))\n",
    "\n",
    "# Define functions for stopwords, bigrams, trigrams and lemmatization\n",
    "stop_words = stopwords.words('english')\n",
    "def remove_stopwords(texts):\n",
    "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "\n",
    "def make_bigrams(texts):\n",
    "    return [bigram_mod[doc] for doc in texts]\n",
    "\n",
    "def make_trigrams(texts):\n",
    "    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "\n",
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return texts_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the text\n",
    "df_text = df[['text']]\n",
    "#df_title = df['title']\n",
    "\n",
    "# remove punctuation and numbers using parallel_apply\n",
    "df_text['text_cleaned'] = df_text['text'].parallel_apply(lambda x: re.sub('[^A-Za-z]+', ' ', x))\n",
    "#df_title['title_cleaned'] = df_title.parallel_apply(lambda x: re.sub('[^A-Za-z]+', ' ', x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop na and duplicates\n",
    "df_text = df_text.dropna().drop_duplicates()\n",
    "# convert to str type\n",
    "df_text['text_cleaned'] = df_text['text_cleaned'].astype(str)\n",
    "df_text.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save a copy\n",
    "df_text_before = df_text.copy()\n",
    "\n",
    "# define a function to handle errors\n",
    "def handle_errors(func):\n",
    "    def wrapper(x):\n",
    "        try:\n",
    "            return func(x)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing row: {x}\")\n",
    "            return np.nan\n",
    "    return wrapper\n",
    "\n",
    "# define the remove_stopwords function with the handle_errors decorator\n",
    "@handle_errors\n",
    "def remove_stopwords(row): \n",
    "    return [i for i in simple_preprocess(row) if i not in stopwords.words('english')]\n",
    "\n",
    "# apply remove_stopwords function with try/except\n",
    "df_text['text_cleaned'] = df_text['text_cleaned'].parallel_apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_text.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save checkpoint\n",
    "checkpoint_path = '/home/jupyter/data/nlp_final'+'df_text_0514_lda.parquet'\n",
    "#df_text.to_parquet(checkpoint_path, engine='pyarrow')\n",
    "\n",
    "# read data\n",
    "#df_text = pd.read_parquet(checkpoint_path, engine='pyarrow')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### titleza LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# tokenize the text\n",
    "data_list = df_text['text_cleaned'].tolist()\n",
    "data_tokens = list(sent_to_words(data_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# create bigrams & trigrams\n",
    "bigram = gensim.models.Phrases(data_tokens, min_count=1, threshold=1)\n",
    "trigram = gensim.models.Phrases(bigram[data_tokens], threshold=1)\n",
    "\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Remove Stop Words\n",
    "#data_tokens_nostops = remove_stopwords(data_tokens)\n",
    "\n",
    "# Create n-grams\n",
    "data_words_bigrams = make_bigrams(data_tokens)\n",
    "data_words_trigrams = make_trigrams(data_tokens)\n",
    "\n",
    "# Combine tokens and n-grams\n",
    "# data_tokens_cobnined = data_tokens_nostops + data_words_bigrams + data_words_trigrams\n",
    "data_tokens_cobnined = data_words_trigrams\n",
    "\n",
    "# Lemmatize text keeping only noun, adj, vb, adv\n",
    "data_lemmatized = lemmatization(data_tokens_cobnined, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
    "\n",
    "print(*data_lemmatized[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = '/home/jupyter/data/nlp_final'\n",
    "\n",
    "# # save the limmatized data to txt\n",
    "# path = checkpoint_path + '/' + 'data_lemmatized.txt'\n",
    "# with open(path, 'w') as f:\n",
    "#     for item in data_lemmatized:\n",
    "#         f.write(\"%s\\n\" % item)\n",
    "\n",
    "# read the limmatized data from txt\n",
    "path = checkpoint_path + '/' + 'data_lemmatized.txt'\n",
    "with open(path, 'r') as f:\n",
    "    data_read = f.read().splitlines()\n",
    "\n",
    "# read each element (a str) of data_read to a list; append to data_lemmatized\n",
    "data_lemmatized = []\n",
    "\n",
    "def string_to_list(str):\n",
    "    # revemo al \" and []\n",
    "    str = str.replace('\"', '')\n",
    "    str = str.replace('\\'', '')\n",
    "    str = str.replace('[', '')\n",
    "    str = str.replace(']', '')\n",
    "    # split by ', '\n",
    "    output_list = str.split(', ')\n",
    "    return output_list\n",
    "\n",
    "for i in range(len(data_read)):\n",
    "    data_lemmatized.append(string_to_list(data_read[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Creating the term dictionary of our courpus, where every unique term is assigned an index. \n",
    "dictionary = corpora.Dictionary(data_lemmatized)\n",
    "\n",
    "# Converting list of documents (corpus) into Document Term Matrix using dictionary prepared above.\n",
    "doc_term_matrix = [dictionary.doc2bow(doc) for doc in data_lemmatized]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_processors = multiprocessing.cpu_count()\n",
    "workers = num_processors-1\n",
    "print(f'Using {workers} workers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# supporting function\n",
    "def compute_coherence_values(corpus, dictionary, k, a, b):\n",
    "    \n",
    "    lda_model = LdaMulticore(corpus=doc_term_matrix,\n",
    "                       id2word=dictionary,\n",
    "                       num_topics=k,\n",
    "                       random_state=100,                  \n",
    "                       passes=10,\n",
    "                       alpha=a,\n",
    "                       eta=b,\n",
    "                       workers=workers)\n",
    "    \n",
    "    coherence_model_lda = CoherenceModel(model=lda_model, texts=data_lemmatized, dictionary=dictionary, coherence='c_v')\n",
    "    \n",
    "    return coherence_model_lda.get_coherence()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = {}\n",
    "grid['Validation_Set'] = {}\n",
    "\n",
    "# Topics range\n",
    "min_topics = 2\n",
    "max_topics = 10\n",
    "step_size = 1\n",
    "topics_range = range(min_topics, max_topics+1, step_size)\n",
    "\n",
    "# Alpha parameter\n",
    "alpha = ['asymmetric'] # Run for number of topics only\n",
    "\n",
    "# Beta parameter\n",
    "beta = ['auto'] # Run for number of topics only\n",
    "\n",
    "# Validation sets\n",
    "num_of_docs = len(doc_term_matrix)\n",
    "corpus_sets = [# gensim.utils.ClippedCorpus(doc_term_matrix, num_of_docs*0.25), \n",
    "               # gensim.utils.ClippedCorpus(doc_term_matrix, num_of_docs*0.5), \n",
    "               # gensim.utils.ClippedCorpus(doc_term_matrix, num_of_docs*0.75), \n",
    "               doc_term_matrix]\n",
    "\n",
    "corpus_title = ['100% Corpus']\n",
    "model_results = {\n",
    "                 'Topics': [],\n",
    "                 'Alpha': [],\n",
    "                 'Beta': [],\n",
    "                 'Coherence': []\n",
    "                }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''%%time \n",
    "\n",
    "itr = 0\n",
    "itr_total = len(beta)*len(alpha)*len(topics_range)*len(corpus_title)\n",
    "print(f'LDA will execute {itr_total} iterations')\n",
    "\n",
    "# iterate through hyperparameters\n",
    "for i in tqdm(range(len(corpus_sets))):\n",
    "    # iterate through number of topics\n",
    "    for k in topics_range:\n",
    "        # iterate through alpha values\n",
    "        #tic()\n",
    "        for a in alpha:\n",
    "            # iterare through beta values\n",
    "            for b in beta:\n",
    "                # get the coherence score for the given parameters\n",
    "                itr += 1\n",
    "                cv = compute_coherence_values(corpus=corpus_sets[i], dictionary=dictionary,\n",
    "                                              k=k, a=a, b=b)\n",
    "                # Save the model results\n",
    "                model_results['Topics'].append(k)\n",
    "                model_results['Alpha'].append(a)\n",
    "                model_results['Beta'].append(b)\n",
    "                model_results['Coherence'].append(cv)\n",
    "                pct_completed = round((itr / itr_total * 100),1)\n",
    "        print(f'Completed model based on {k} LDA topics. Finished {pct_completed}% of LDA runs')\n",
    "\n",
    "lda_tuning = pd.DataFrame(model_results)     '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed\n",
    "\n",
    "def compute_coherence_values(corpus, dictionary, k, a, b):\n",
    "    lda_model = LdaMulticore(corpus=corpus,\n",
    "                       id2word=dictionary,\n",
    "                       num_topics=k,\n",
    "                       random_state=100,                  \n",
    "                       passes=10,\n",
    "                       alpha=a,\n",
    "                       eta=b,\n",
    "                       workers=workers)\n",
    "    coherence_model_lda = CoherenceModel(model=lda_model, texts=data_lemmatized, dictionary=dictionary, coherence='c_v')\n",
    "    return coherence_model_lda.get_coherence()\n",
    "\n",
    "itr = 0\n",
    "itr_total = len(beta)*len(alpha)*len(topics_range)*len(corpus_title)\n",
    "print(f'LDA will execute {itr_total} iterations')\n",
    "\n",
    "# iterate through hyperparameters\n",
    "for i in tqdm(range(len(corpus_sets))):\n",
    "    # iterate through number of topics\n",
    "    for k in topics_range:\n",
    "        # iterate through alpha values\n",
    "        #tic()\n",
    "        for a in alpha:\n",
    "            # iterate through beta values\n",
    "            jobs = []\n",
    "            for b in beta:\n",
    "                itr += 1\n",
    "                # run each beta value in parallel\n",
    "                job = delayed(compute_coherence_values)(corpus=corpus_sets[i], dictionary=dictionary,\n",
    "                                                  k=k, a=a, b=b)\n",
    "                jobs.append(job)\n",
    "            # compute coherence scores in parallel and save results\n",
    "            coherences = Parallel(n_jobs=min(8, len(jobs)))(jobs)\n",
    "            for b, cv in zip(beta, coherences):\n",
    "                model_results['Topics'].append(k)\n",
    "                model_results['Alpha'].append(a)\n",
    "                model_results['Beta'].append(b)\n",
    "                model_results['Coherence'].append(cv)\n",
    "            pct_completed = round((itr / itr_total * 100),1)\n",
    "        print(f'Completed model based on {k} LDA topics. Finished {pct_completed}% of LDA runs')\n",
    "\n",
    "lda_tuning = pd.DataFrame(model_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best LDA parameters\n",
    "lda_tuning.sort_values(by=['Coherence'], ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_tuning.plot(x ='Topics', y='Coherence', kind = 'line', xticks=range(1,20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_tuning_best = lda_tuning.sort_values(by=['Coherence'], ascending=False).head(1)\n",
    "\n",
    "\n",
    "tuned_topics = int(lda_tuning_best['Topics'].to_string(index=False))\n",
    "\n",
    "\n",
    "# Since the values for Alpha and Beta can be float, symmetric and asymmetric, we will either strip or convert to float\n",
    "try:\n",
    "    tuned_alpha = float(lda_tuning_best['Alpha'].to_string(index=False))\n",
    "except:\n",
    "    tuned_alpha = lda_tuning_best['Alpha'].to_string(index=False).strip()\n",
    "    \n",
    "\n",
    "try:\n",
    "    tuned_beta = float(lda_tuning_best['Beta'].to_string(index=False))\n",
    "except:\n",
    "    tuned_beta = lda_tuning_best['Beta'].to_string(index=False).strip()    \n",
    "    \n",
    "print(f'Best Parameters: Topics: {tuned_topics}, Alpha: {tuned_alpha}, Beta: {tuned_beta}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "tuned_lda_model = LdaMulticore(corpus=doc_term_matrix,\n",
    "                       id2word=dictionary,\n",
    "                       #num_topics=tuned_topics,\n",
    "                       num_topics=4,\n",
    "                       random_state=100,\n",
    "                       passes=10,\n",
    "                       alpha=tuned_alpha,\n",
    "                       eta=tuned_beta,\n",
    "                       workers = workers)\n",
    "\n",
    "coherence_model_lda = CoherenceModel(model=tuned_lda_model, texts=data_lemmatized, dictionary=dictionary, coherence='c_v')\n",
    "print('\\nCoherence Score: ', coherence_model_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "lda_display = gensimvis.prepare(tuned_lda_model, doc_term_matrix, dictionary, sort_topics=False, mds='mmds')\n",
    "pyLDAvis.display(lda_display)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
